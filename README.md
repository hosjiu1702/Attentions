# Attentions
Implement the Attention in Neural Network

### Todos
- [x] Multi Headed Attention [(https://arxiv.org/abs/1706.03762)](https://arxiv.org/abs/1706.03762)
- [] Flash Attention [(https://arxiv.org/abs/2205.14135)](https://arxiv.org/abs/2205.14135)